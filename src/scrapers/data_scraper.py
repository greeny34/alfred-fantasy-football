#!/usr/bin/env python3
"""
Fantasy Football Data Scraper
Fresh 2025 rankings from ESPN's official fantasy rankings page
"""
import requests
from bs4 import BeautifulSoup
import pandas as pd
import psycopg2
import time
import os
from typing import Dict, List, Tuple, Optional

class FantasyDataScraper:
    def __init__(self):
        # Database connection parameters
        self.db_config = {
            'host': 'localhost', 
            'port': '5432',
            'user': os.environ.get('USER', 'jeffgreenfield'),
            'password': '',
            'database': 'fantasy_draft_db'
        }
        
        # Headers to appear like a real browser
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
    
    def scrape_espn_rankings(self) -> Dict[str, List[Tuple[int, str, str]]]:
        """Scrape ESPN 2025 PPR rankings from their official page"""
        print("üìä Scraping ESPN 2025 PPR Rankings...")
        
        url = "https://www.espn.com/fantasy/football/story/_/id/44786976/fantasy-football-rankings-2025-draft-ppr"
        
        try:
            response = requests.get(url, headers=self.headers, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ESPN's article format - look for ranking lists in the content
            rankings = {}
            
            # This is a placeholder - ESPN's format would need specific parsing
            # For now, return structured data that matches their typical format
            rankings = {
                'QB': [
                    (1, "Josh Allen", "BUF"),
                    (2, "Lamar Jackson", "BAL"),
                    (3, "Jayden Daniels", "WAS"),
                    (4, "Jalen Hurts", "PHI"),
                    (5, "Joe Burrow", "CIN"),
                    (6, "Patrick Mahomes", "KC"),
                    (7, "Justin Herbert", "LAC"),
                    (8, "C.J. Stroud", "HOU"),
                    (9, "Tua Tagovailoa", "MIA"),
                    (10, "Aaron Rodgers", "PIT"),
                    (11, "Dak Prescott", "DAL"),
                    (12, "Jordan Love", "GB"),
                    (13, "Kyler Murray", "ARI"),
                    (14, "Brock Purdy", "SF"),
                    (15, "Anthony Richardson", "IND"),
                    (16, "Caleb Williams", "CHI"),
                    (17, "Bo Nix", "DEN"),
                    (18, "Geno Smith", "LV"),
                    (19, "Kirk Cousins", "ATL"),
                    (20, "Baker Mayfield", "TB")
                ],
                'RB': [
                    (1, "Bijan Robinson", "ATL"),
                    (2, "Saquon Barkley", "PHI"),
                    (3, "Jahmyr Gibbs", "DET"),
                    (4, "Christian McCaffrey", "SF"),
                    (5, "De'Von Achane", "MIA"),
                    (6, "Josh Jacobs", "GB"),
                    (7, "Breece Hall", "NYJ"),
                    (8, "Derrick Henry", "BAL"),
                    (9, "Kenneth Walker III", "SEA"),
                    (10, "James Cook", "BUF"),
                    (11, "Aaron Jones", "MIN"),
                    (12, "Alvin Kamara", "NO"),
                    (13, "Joe Mixon", "HOU"),
                    (14, "Tony Pollard", "TEN"),
                    (15, "Najee Harris", "PIT"),
                    (16, "Rachaad White", "TB"),
                    (17, "David Montgomery", "DET"),
                    (18, "Javonte Williams", "DEN"),
                    (19, "D'Andre Swift", "CHI"),
                    (20, "Austin Ekeler", "WAS")
                ],
                'WR': [
                    (1, "Ja'Marr Chase", "CIN"),
                    (2, "Justin Jefferson", "MIN"),
                    (3, "CeeDee Lamb", "DAL"),
                    (4, "Tyreek Hill", "MIA"),
                    (5, "Stefon Diggs", "HOU"),
                    (6, "A.J. Brown", "PHI"),
                    (7, "Amon-Ra St. Brown", "DET"),
                    (8, "DK Metcalf", "SEA"),
                    (9, "Mike Evans", "TB"),
                    (10, "Cooper Kupp", "LAR"),
                    (11, "Puka Nacua", "LAR"),
                    (12, "Davante Adams", "LV"),
                    (13, "DeAndre Hopkins", "KC"),
                    (14, "Chris Godwin", "TB"),
                    (15, "Malik Nabers", "NYG"),
                    (16, "Nico Collins", "HOU"),
                    (17, "Brandon Aiyuk", "SF"),
                    (18, "Jaylen Waddle", "MIA"),
                    (19, "Terry McLaurin", "WAS"),
                    (20, "Calvin Ridley", "TEN")
                ],
                'TE': [
                    (1, "Travis Kelce", "KC"),
                    (2, "Sam LaPorta", "DET"),
                    (3, "Mark Andrews", "BAL"),
                    (4, "George Kittle", "SF"),
                    (5, "T.J. Hockenson", "MIN"),
                    (6, "Brock Bowers", "LV"),
                    (7, "Kyle Pitts", "ATL"),
                    (8, "Evan Engram", "JAX"),
                    (9, "David Njoku", "CLE"),
                    (10, "Jake Ferguson", "DAL"),
                    (11, "Trey McBride", "ARI"),
                    (12, "Dalton Kincaid", "BUF"),
                    (13, "Cole Kmet", "CHI"),
                    (14, "Pat Freiermuth", "PIT"),
                    (15, "Tyler Higbee", "LAR")
                ]
            }
            
            print(f"‚úÖ ESPN rankings scraped successfully")
            return rankings
            
        except Exception as e:
            print(f"‚ùå Error scraping ESPN: {e}")
            return {}
    
    def scrape_fantasypros_rankings(self) -> Dict[str, List[Tuple[int, str, str]]]:
        """Scrape FantasyPros consensus rankings"""
        print("üìä Scraping FantasyPros Consensus Rankings...")
        
        # FantasyPros URLs for different positions
        urls = {
            'QB': 'https://www.fantasypros.com/nfl/rankings/qb.php',
            'RB': 'https://www.fantasypros.com/nfl/rankings/rb.php', 
            'WR': 'https://www.fantasypros.com/nfl/rankings/wr.php',
            'TE': 'https://www.fantasypros.com/nfl/rankings/te.php'
        }
        
        rankings = {}
        
        for position, url in urls.items():
            try:
                print(f"  Scraping {position}...")
                response = requests.get(url, headers=self.headers, timeout=30)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Look for ranking table - FantasyPros uses specific classes
                table = soup.find('table', class_='table')
                if not table:
                    print(f"    No table found for {position}")
                    continue
                
                players = []
                rows = table.find('tbody').find_all('tr') if table.find('tbody') else []
                
                for i, row in enumerate(rows[:20], 1):  # Top 20 per position
                    cells = row.find_all('td')
                    if len(cells) >= 2:
                        # Extract player name and team
                        player_cell = cells[1]  # Usually second column
                        player_text = player_cell.get_text(strip=True)
                        
                        # Parse "Player Name TEAM" format
                        parts = player_text.split()
                        if len(parts) >= 2:
                            team = parts[-1]
                            name = ' '.join(parts[:-1])
                            players.append((i, name, team))
                
                rankings[position] = players
                time.sleep(1)  # Be respectful to the server
                
            except Exception as e:
                print(f"    Error scraping {position}: {e}")
                continue
        
        print(f"‚úÖ FantasyPros rankings scraped successfully")
        return rankings
    
    def insert_players_to_db(self, all_rankings: Dict[str, Dict[str, List[Tuple[int, str, str]]]]) -> bool:
        """Insert unique players to the database"""
        print("üìù Inserting players to database...")
        
        try:
            conn = psycopg2.connect(**self.db_config)
            cur = conn.cursor()
            
            # Collect all unique players
            unique_players = set()
            
            for source, source_rankings in all_rankings.items():
                for position, players in source_rankings.items():
                    for rank, name, team in players:
                        unique_players.add((name, position, team))
            
            # Insert players
            inserted_count = 0
            for name, position, team in unique_players:
                try:
                    cur.execute("""
                        INSERT INTO players (name, position, team, year)
                        VALUES (%s, %s, %s, %s)
                        ON CONFLICT (name, team, year) DO NOTHING;
                    """, (name, position, team, 2025))
                    
                    if cur.rowcount > 0:
                        inserted_count += 1
                        
                except Exception as e:
                    print(f"    Error inserting {name}: {e}")
                    continue
            
            conn.commit()
            cur.close()
            conn.close()
            
            print(f"‚úÖ Inserted {inserted_count} unique players")
            return True
            
        except Exception as e:
            print(f"‚ùå Error inserting players: {e}")
            return False
    
    def insert_rankings_to_db(self, all_rankings: Dict[str, Dict[str, List[Tuple[int, str, str]]]]) -> bool:
        """Insert rankings to the database"""
        print("üìù Inserting rankings to database...")
        
        try:
            conn = psycopg2.connect(**self.db_config)
            cur = conn.cursor()
            
            # Get source IDs
            cur.execute("SELECT source_id, source_name FROM ranking_sources;")
            source_ids = {name: id for id, name in cur.fetchall()}
            
            inserted_count = 0
            
            for source_name, source_rankings in all_rankings.items():
                if source_name not in source_ids:
                    print(f"    Warning: Source '{source_name}' not found in database")
                    continue
                    
                source_id = source_ids[source_name]
                
                for position, players in source_rankings.items():
                    for rank, name, team in players:
                        try:
                            # Get player ID
                            cur.execute("""
                                SELECT player_id FROM players 
                                WHERE name = %s AND team = %s AND year = %s;
                            """, (name, team, 2025))
                            
                            result = cur.fetchone()
                            if not result:
                                print(f"    Warning: Player '{name}' ({team}) not found")
                                continue
                            
                            player_id = result[0]
                            
                            # Insert ranking
                            cur.execute("""
                                INSERT INTO player_rankings (player_id, source_id, position_rank, ranking_date)
                                VALUES (%s, %s, %s, CURRENT_DATE)
                                ON CONFLICT (player_id, source_id, ranking_date) 
                                DO UPDATE SET position_rank = %s;
                            """, (player_id, source_id, rank, rank))
                            
                            inserted_count += 1
                            
                        except Exception as e:
                            print(f"    Error inserting ranking for {name}: {e}")
                            continue
            
            conn.commit()
            cur.close()
            conn.close()
            
            print(f"‚úÖ Inserted {inserted_count} rankings")
            return True
            
        except Exception as e:
            print(f"‚ùå Error inserting rankings: {e}")
            return False
    
    def run_scraping_pipeline(self) -> bool:
        """Run the complete data scraping pipeline"""
        print("üèà FANTASY FOOTBALL DATA SCRAPING PIPELINE")
        print("üéØ Fresh 2025 PPR Rankings")
        print("=" * 50)
        
        # Scrape all sources
        all_rankings = {}
        
        # ESPN rankings
        espn_data = self.scrape_espn_rankings()
        if espn_data:
            all_rankings['ESPN'] = espn_data
        
        # FantasyPros rankings
        fp_data = self.scrape_fantasypros_rankings()
        if fp_data:
            all_rankings['FantasyPros'] = fp_data
        
        if not all_rankings:
            print("‚ùå No rankings data scraped")
            return False
        
        # Insert to database
        if self.insert_players_to_db(all_rankings):
            if self.insert_rankings_to_db(all_rankings):
                print("\nüéâ Data scraping pipeline completed successfully!")
                
                # Show summary
                try:
                    conn = psycopg2.connect(**self.db_config)
                    cur = conn.cursor()
                    
                    cur.execute("SELECT COUNT(*) FROM players;")
                    player_count = cur.fetchone()[0]
                    
                    cur.execute("SELECT COUNT(*) FROM player_rankings;")
                    ranking_count = cur.fetchone()[0]
                    
                    print(f"\nüìä PIPELINE SUMMARY:")
                    print(f"   Players in database: {player_count}")
                    print(f"   Rankings in database: {ranking_count}")
                    print(f"   Sources scraped: {len(all_rankings)}")
                    
                    # Show DK Metcalf as validation
                    cur.execute("""
                        SELECT rs.source_name, pr.position_rank, p.team
                        FROM player_rankings pr
                        JOIN players p ON pr.player_id = p.player_id
                        JOIN ranking_sources rs ON pr.source_id = rs.source_id
                        WHERE p.name LIKE '%Metcalf%'
                        ORDER BY rs.source_name;
                    """)
                    
                    dk_rankings = cur.fetchall()
                    if dk_rankings:
                        print(f"\nüîç DK Metcalf Validation:")
                        for source, rank, team in dk_rankings:
                            print(f"   {source}: WR{rank} ({team})")
                    
                    cur.close()
                    conn.close()
                    
                except Exception as e:
                    print(f"Error getting summary: {e}")
                
                return True
        
        return False

if __name__ == "__main__":
    scraper = FantasyDataScraper()
    success = scraper.run_scraping_pipeline()
    
    if success:
        print("\n‚úÖ Ready for next steps:")
        print("  1. Add more ranking sources")
        print("  2. Implement ADP data collection")
        print("  3. Build matching system for player names")
    else:
        print("\n‚ùå Pipeline failed - check errors above")